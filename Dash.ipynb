{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dash\n",
    "\n",
    "## Load libraries\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import importlib\r\n",
    "\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "import torch.optim as optim\r\n",
    "from torch.utils import data\r\n",
    "from torchvision import transforms\r\n",
    "import distiller.apputils as apputils\r\n",
    "\r\n",
    "import sys\r\n",
    "\r\n",
    "import ai8x\r\n",
    "\r\n",
    "kws20 = importlib.import_module(\"datasets.kws20\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Calculate the weights of the classes relative to the sample size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pathlib import Path\r\n",
    "\r\n",
    "raw_data_path = Path('data/KWS_DASH/raw')\r\n",
    "class_file_count = {}\r\n",
    "\r\n",
    "class_dirs = [d for d in raw_data_path.iterdir() if d.is_dir() and d.stem != \"_background_noise_\"]\r\n",
    "\r\n",
    "\r\n",
    "for d in class_dirs:\r\n",
    "    class_file_count[d] = len(list(d.iterdir()))\r\n",
    "\r\n",
    "min_file_count = float(min(class_file_count.values()))\r\n",
    "\r\n",
    "for d in class_dirs:\r\n",
    "    class_file_count[d] = min_file_count / class_file_count[d]\r\n",
    "    print(f\"{d.stem}: {round(class_file_count[d], 7)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "blinds: 1.0\n",
      "dash: 1.0\n",
      "down: 0.0079142\n",
      "energy: 0.96875\n",
      "lights: 0.96875\n",
      "off: 0.0082777\n",
      "on: 0.0080624\n",
      "up: 0.0083266\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate processed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_set, test_set = kws20.KWS_DASH_get_datasets(('data', ''))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No key `noise_var` in input augmentation dictionary!  Using defaults: [Min: 0., Max: 1.]\n",
      "No key `shift` in input augmentation dictionary! Using defaults: [Min:-0.1, Max: 0.1]\n",
      "No key `strech` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]\n",
      "\n",
      "Processing train...\n",
      "Class blinds (# 0): 81 elements\n",
      "Class dash (# 1): 81 elements\n",
      "Class down (# 2): 10665 elements\n",
      "Class energy (# 3): 84 elements\n",
      "Class lights (# 4): 84 elements\n",
      "Class off (# 5): 10158 elements\n",
      "Class on (# 6): 10464 elements\n",
      "Class up (# 7): 10152 elements\n",
      "Class UNKNOWN: 0 elements\n",
      "\n",
      "Processing test...\n",
      "Class blinds (# 0): 12 elements\n",
      "Class dash (# 1): 12 elements\n",
      "Class down (# 2): 1086 elements\n",
      "Class energy (# 3): 12 elements\n",
      "Class lights (# 4): 12 elements\n",
      "Class off (# 5): 1077 elements\n",
      "Class on (# 6): 1071 elements\n",
      "Class up (# 7): 1017 elements\n",
      "Class UNKNOWN: 0 elements\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_batch_size = 32\r\n",
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\r\n",
    "    kws20.KWS_DASH_get_datasets, (\"data\", None), train_batch_size, 1)\r\n",
    "print(f\"Dataset sizes:\\n\\ttraining={len(train_loader.sampler)}\\n\\tvalidation={len(val_loader.sampler)}\\n\\ttest={len(test_loader.sampler)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No key `noise_var` in input augmentation dictionary!  Using defaults: [Min: 0., Max: 1.]\n",
      "No key `shift` in input augmentation dictionary! Using defaults: [Min:-0.1, Max: 0.1]\n",
      "No key `strech` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]\n",
      "\n",
      "Processing train...\n",
      "Class blinds (# 0): 81 elements\n",
      "Class dash (# 1): 81 elements\n",
      "Class down (# 2): 10665 elements\n",
      "Class energy (# 3): 84 elements\n",
      "Class lights (# 4): 84 elements\n",
      "Class off (# 5): 10158 elements\n",
      "Class on (# 6): 10464 elements\n",
      "Class up (# 7): 10152 elements\n",
      "Class UNKNOWN: 0 elements\n",
      "\n",
      "Processing test...\n",
      "Class blinds (# 0): 12 elements\n",
      "Class dash (# 1): 12 elements\n",
      "Class down (# 2): 1086 elements\n",
      "Class energy (# 3): 12 elements\n",
      "Class lights (# 4): 12 elements\n",
      "Class off (# 5): 1077 elements\n",
      "Class on (# 6): 1071 elements\n",
      "Class up (# 7): 1017 elements\n",
      "Class UNKNOWN: 0 elements\n",
      "Dataset sizes:\n",
      "\ttraining=37593\n",
      "\tvalidation=4176\n",
      "\ttest=4299\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
    "print('Running on device: {}'.format(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    class_dict = {'backward': 0, 'bed': 1, 'bird': 2, 'cat': 3, 'dog': 4, 'down': 5,\r\n",
    "                  'eight': 6, 'five': 7, 'follow': 8, 'forward': 9, 'four': 10, 'go': 11,\r\n",
    "                  'happy': 12, 'house': 13, 'learn': 14, 'left': 15, 'marvin': 16, 'nine': 17,\r\n",
    "                  'no': 18, 'off': 19, 'on': 20, 'one': 21, 'right': 22, 'seven': 23,\r\n",
    "                  'sheila': 24, 'six': 25, 'stop': 26, 'three': 27, 'tree': 28, 'two': 29,\r\n",
    "                  'up': 30, 'visual': 31, 'wow': 32, 'yes': 33, 'zero': 34, 'dash': 35, 'energy': 36, \r\n",
    "                  'lights': 37, 'blinds': 38}\r\n",
    "    classes = list(class_dict.keys())\r\n",
    "    classes.sort()\r\n",
    "    for i, c in enumerate(classes):\r\n",
    "        print(f\"\\\"{c}\\\": {i},\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load pretrained model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def count_params(model):\r\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\r\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\r\n",
    "    return params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\r\n",
    "\r\n",
    "mod = importlib.import_module(\"models.ai85net-kws20-v3\")\r\n",
    "\r\n",
    "model = mod.AI85KWS20Netv3(num_classes=21, num_channels=128, dimensions=(128, 1), bias=False)\r\n",
    "print(f'Number of Model Params: {count_params(model)}')\r\n",
    "\r\n",
    "model, compression_scheduler, optimizer, start_epoch = apputils.load_checkpoint(\r\n",
    "            model, \"logs/2022.05.20-235449/qat_checkpoint.pth.tar\", model_device='cuda')\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuring device: MAX78000, simulate=False.\n",
      "Number of Model Params: 169472\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replace FC layer and freeze the rest of the layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def freeze_layer(layer):\r\n",
    "    for p in layer.parameters():\r\n",
    "        p.requires_grad = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "freeze_layer(model.voice_conv1)\r\n",
    "freeze_layer(model.voice_conv2)\r\n",
    "freeze_layer(model.voice_conv3)\r\n",
    "freeze_layer(model.voice_conv4)\r\n",
    "freeze_layer(model.kws_conv1)\r\n",
    "freeze_layer(model.kws_conv2)\r\n",
    "freeze_layer(model.kws_conv3)\r\n",
    "freeze_layer(model.kws_conv4)\r\n",
    "model.fc = ai8x.Linear(256, 8, bias=False, wide=True)\r\n",
    "\r\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "num_epochs = 50\r\n",
    "epoch = 0\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
    "ms_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 80], gamma=0.5)\r\n",
    "criterion = torch.nn.CrossEntropyLoss(\r\n",
    "    torch.Tensor((1, 1, 0.01, 1, 1, 0.01, 0.01, 0.01))\r\n",
    ")\r\n",
    "criterion.to(device)\r\n",
    "\r\n",
    "qat_policy = {\r\n",
    "    'start_epoch': 20,\r\n",
    "    'weight_bits': 8\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "best_acc = 0\r\n",
    "best_qat_acc = 0\r\n",
    "for epoch in range(0, num_epochs):\r\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\r\n",
    "        print('QAT is starting!')\r\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\r\n",
    "        ai8x.fuse_bn_layers(model)\r\n",
    "\r\n",
    "        # Switch model from unquantized to quantized for QAT\r\n",
    "        ai8x.initiate_qat(model, qat_policy)\r\n",
    "\r\n",
    "        # Model is re-transferred to GPU in case parameters were added\r\n",
    "        model.to(device)\r\n",
    "    running_loss = []\r\n",
    "    train_start = time.time()\r\n",
    "    model.train()\r\n",
    "    for idx, (inputs, target) in enumerate(train_loader):\r\n",
    "        inputs = inputs.to(device)\r\n",
    "        target = target.to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        model_out = model(inputs)\r\n",
    "        \r\n",
    "        loss = criterion(model_out, target)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss.append(loss.cpu().detach().numpy())\r\n",
    "\r\n",
    "    mean_loss = np.mean(running_loss)\r\n",
    "    train_end = time.time()\r\n",
    "    print(\"Epoch: {}/{}\\t LR: {}\\t Train Loss: {:.4f}\\t Dur: {:.2f} sec.\".format(\r\n",
    "        epoch+1, num_epochs, ms_lr_scheduler.get_lr(), mean_loss, (train_end-train_start)))\r\n",
    "    \r\n",
    "    model.eval()\r\n",
    "    acc = 0.\r\n",
    "    acc_weight = 0\r\n",
    "    with torch.no_grad():\r\n",
    "        for inputs, target in test_loader:\r\n",
    "            inputs = inputs.to(device)\r\n",
    "            target = target.to(device)\r\n",
    "            model_out = model(inputs)\r\n",
    "            target_out = torch.argmax(model_out, dim=1)\r\n",
    "            \r\n",
    "            tp = torch.sum(target_out == target)\r\n",
    "            acc_batch = (tp / target_out.numel()).detach().item()\r\n",
    "            acc += target_out.shape[0] * acc_batch\r\n",
    "            acc_weight += target_out.shape[0]\r\n",
    "            \r\n",
    "        total_acc = 100 * (acc / acc_weight)\r\n",
    "        if epoch == qat_policy['start_epoch']: best_acc = 0\r\n",
    "        if total_acc > best_acc:\r\n",
    "            best_acc = total_acc\r\n",
    "            checkpoint_extras = {'current_top1': best_acc,\r\n",
    "                                 'best_top1': best_acc,\r\n",
    "                                 'best_epoch': epoch}\r\n",
    "            model_name = 'ai85net_kws_dash'\r\n",
    "            model_prefix = f'{model_name}' if epoch < qat_policy['start_epoch'] else (f'qat_{model_name}')\r\n",
    "            apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\r\n",
    "                                     scheduler=None, extras=checkpoint_extras,\r\n",
    "                                     is_best=True, name=model_prefix,\r\n",
    "                                     dir='.')\r\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\r\n",
    "            \r\n",
    "        print('\\t\\t Test Acc: {:.2f}'.format(total_acc))\r\n",
    "    ms_lr_scheduler.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/50\t LR: [0.001]\t Train Loss: 0.4758\t Dur: 24.67 sec.\n",
      "Best model saved with accuracy: 89.02%\n",
      "\t\t Test Acc: 89.02\n",
      "Epoch: 2/50\t LR: [0.001]\t Train Loss: 0.4319\t Dur: 20.29 sec.\n",
      "\t\t Test Acc: 88.81\n",
      "Epoch: 3/50\t LR: [0.001]\t Train Loss: 0.4262\t Dur: 22.08 sec.\n",
      "Best model saved with accuracy: 89.39%\n",
      "\t\t Test Acc: 89.39\n",
      "Epoch: 4/50\t LR: [0.001]\t Train Loss: 0.4199\t Dur: 25.70 sec.\n",
      "\t\t Test Acc: 88.58\n",
      "Epoch: 5/50\t LR: [0.001]\t Train Loss: 0.4142\t Dur: 35.91 sec.\n",
      "\t\t Test Acc: 88.81\n",
      "Epoch: 6/50\t LR: [0.001]\t Train Loss: 0.4123\t Dur: 28.17 sec.\n",
      "\t\t Test Acc: 89.11\n",
      "Epoch: 7/50\t LR: [0.001]\t Train Loss: 0.4072\t Dur: 27.57 sec.\n",
      "\t\t Test Acc: 88.74\n",
      "Epoch: 8/50\t LR: [0.001]\t Train Loss: 0.4092\t Dur: 25.46 sec.\n",
      "\t\t Test Acc: 88.51\n",
      "Epoch: 9/50\t LR: [0.001]\t Train Loss: 0.3908\t Dur: 30.96 sec.\n",
      "\t\t Test Acc: 89.32\n",
      "Epoch: 10/50\t LR: [0.001]\t Train Loss: 0.3900\t Dur: 30.26 sec.\n",
      "\t\t Test Acc: 88.88\n",
      "Epoch: 11/50\t LR: [0.001]\t Train Loss: 0.4075\t Dur: 31.53 sec.\n",
      "\t\t Test Acc: 89.35\n",
      "Epoch: 12/50\t LR: [0.001]\t Train Loss: 0.4091\t Dur: 32.42 sec.\n",
      "\t\t Test Acc: 88.79\n",
      "Epoch: 13/50\t LR: [0.001]\t Train Loss: 0.3884\t Dur: 22.54 sec.\n",
      "\t\t Test Acc: 89.37\n",
      "Epoch: 14/50\t LR: [0.001]\t Train Loss: 0.3891\t Dur: 26.29 sec.\n",
      "\t\t Test Acc: 88.56\n",
      "Epoch: 15/50\t LR: [0.001]\t Train Loss: 0.3900\t Dur: 22.95 sec.\n",
      "\t\t Test Acc: 89.21\n",
      "Epoch: 16/50\t LR: [0.001]\t Train Loss: 0.3896\t Dur: 22.30 sec.\n",
      "\t\t Test Acc: 89.30\n",
      "Epoch: 17/50\t LR: [0.001]\t Train Loss: 0.3826\t Dur: 26.64 sec.\n",
      "\t\t Test Acc: 89.14\n",
      "Epoch: 18/50\t LR: [0.001]\t Train Loss: 0.3896\t Dur: 20.98 sec.\n",
      "\t\t Test Acc: 89.16\n",
      "Epoch: 19/50\t LR: [0.001]\t Train Loss: 0.3832\t Dur: 20.77 sec.\n",
      "\t\t Test Acc: 89.23\n",
      "Epoch: 20/50\t LR: [0.001]\t Train Loss: 0.3998\t Dur: 22.87 sec.\n",
      "\t\t Test Acc: 89.09\n",
      "QAT is starting!\n",
      "Epoch: 21/50\t LR: [0.00025]\t Train Loss: 0.3760\t Dur: 29.38 sec.\n",
      "Best model saved with accuracy: 96.05%\n",
      "\t\t Test Acc: 96.05\n",
      "Epoch: 22/50\t LR: [0.0005]\t Train Loss: 0.2559\t Dur: 28.76 sec.\n",
      "\t\t Test Acc: 95.81\n",
      "Epoch: 23/50\t LR: [0.0005]\t Train Loss: 0.2273\t Dur: 26.07 sec.\n",
      "\t\t Test Acc: 95.74\n",
      "Epoch: 24/50\t LR: [0.0005]\t Train Loss: 0.2084\t Dur: 24.37 sec.\n",
      "\t\t Test Acc: 95.70\n",
      "Epoch: 25/50\t LR: [0.0005]\t Train Loss: 0.1982\t Dur: 25.97 sec.\n",
      "\t\t Test Acc: 95.77\n",
      "Epoch: 26/50\t LR: [0.0005]\t Train Loss: 0.1870\t Dur: 27.20 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 27/50\t LR: [0.0005]\t Train Loss: 0.1836\t Dur: 27.18 sec.\n",
      "\t\t Test Acc: 95.84\n",
      "Epoch: 28/50\t LR: [0.0005]\t Train Loss: 0.1835\t Dur: 26.00 sec.\n",
      "\t\t Test Acc: 95.88\n",
      "Epoch: 29/50\t LR: [0.0005]\t Train Loss: 0.1765\t Dur: 24.83 sec.\n",
      "\t\t Test Acc: 95.88\n",
      "Epoch: 30/50\t LR: [0.0005]\t Train Loss: 0.1740\t Dur: 31.08 sec.\n",
      "\t\t Test Acc: 95.95\n",
      "Epoch: 31/50\t LR: [0.0005]\t Train Loss: 0.1702\t Dur: 28.00 sec.\n",
      "\t\t Test Acc: 95.98\n",
      "Epoch: 32/50\t LR: [0.0005]\t Train Loss: 0.1729\t Dur: 25.86 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 33/50\t LR: [0.0005]\t Train Loss: 0.2566\t Dur: 25.19 sec.\n",
      "\t\t Test Acc: 96.00\n",
      "Epoch: 34/50\t LR: [0.0005]\t Train Loss: 0.2334\t Dur: 26.82 sec.\n",
      "\t\t Test Acc: 95.98\n",
      "Epoch: 35/50\t LR: [0.0005]\t Train Loss: 0.2145\t Dur: 27.93 sec.\n",
      "\t\t Test Acc: 95.84\n",
      "Epoch: 36/50\t LR: [0.0005]\t Train Loss: 0.2042\t Dur: 25.58 sec.\n",
      "\t\t Test Acc: 95.98\n",
      "Epoch: 37/50\t LR: [0.0005]\t Train Loss: 0.1977\t Dur: 25.17 sec.\n",
      "\t\t Test Acc: 95.88\n",
      "Epoch: 38/50\t LR: [0.0005]\t Train Loss: 0.1953\t Dur: 24.17 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 39/50\t LR: [0.0005]\t Train Loss: 0.1901\t Dur: 28.25 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 40/50\t LR: [0.0005]\t Train Loss: 0.1829\t Dur: 25.93 sec.\n",
      "\t\t Test Acc: 95.84\n",
      "Epoch: 41/50\t LR: [0.0005]\t Train Loss: 0.1798\t Dur: 24.30 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 42/50\t LR: [0.0005]\t Train Loss: 0.1806\t Dur: 24.84 sec.\n",
      "\t\t Test Acc: 95.84\n",
      "Epoch: 43/50\t LR: [0.0005]\t Train Loss: 0.1784\t Dur: 25.62 sec.\n",
      "\t\t Test Acc: 95.86\n",
      "Epoch: 44/50\t LR: [0.0005]\t Train Loss: 0.1716\t Dur: 27.40 sec.\n",
      "\t\t Test Acc: 95.93\n",
      "Epoch: 45/50\t LR: [0.0005]\t Train Loss: 0.1789\t Dur: 29.34 sec.\n",
      "\t\t Test Acc: 95.91\n",
      "Epoch: 46/50\t LR: [0.0005]\t Train Loss: 0.1705\t Dur: 27.87 sec.\n",
      "\t\t Test Acc: 95.91\n",
      "Epoch: 47/50\t LR: [0.0005]\t Train Loss: 0.1711\t Dur: 26.65 sec.\n",
      "\t\t Test Acc: 95.91\n",
      "Epoch: 48/50\t LR: [0.0005]\t Train Loss: 0.1747\t Dur: 28.99 sec.\n",
      "\t\t Test Acc: 95.81\n",
      "Epoch: 49/50\t LR: [0.0005]\t Train Loss: 0.1680\t Dur: 25.96 sec.\n",
      "\t\t Test Acc: 95.91\n",
      "Epoch: 50/50\t LR: [0.0005]\t Train Loss: 0.1709\t Dur: 24.56 sec.\n",
      "\t\t Test Acc: 95.86\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "2a0540a1e80046da7a13ad6e0f3e2669dbea6f8385b58dacdddeaad8110ec434"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}